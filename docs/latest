<!DOCTYPE html>
<html lang="en"><head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>notech.ie</title>
<style>
body {
  margin-top: 0;
  margin-bottom: 0;
}
#hcontent {
  height: 100vh;
  padding-right: 1vw;
  width: 50em;
  max-width: 98vw;
  overflow: auto;
  resize: horizontal;
}
</style>
</head><body>
<div id=hcontent>
<h1>notech.ie</h1>
<ul>
<li><a href=#optioning>optioning</a> could be another way to have interesting conversations.</li>
<li><a href=#typer>typer</a> is the ideal laptoplike device i'd like to have.</li>
<li><a href=#briefquiz>briefquiz</a> for kids could be a fun game.</li>
<li><a href=#goldentesting>goldentesting</a> is an alternative to a unittesting.</li>
</ul>

<h2 id="optioning">optioning</h2>
<p>
i&apos;m really bad in conversations.
in real life i tend to be too silent and superficial.
in emails i tend to write too much, sometimes being too insensitive.
i&apos;m pretty sure i often overwhelm the recipient with the quantity
because i often don&apos;t get any replies.
and my writing style is not very coherent,
i just keep rambling about random stuff.
reading my writing is probably quite hard.
and often people don&apos;t even like to write,
so then i&apos;m having hard time to communicate with such people.
</p>

<p>
i think there are two big problems with my art of communication.
the quantity problem: i&apos;m tempted to ask too many questions.
it&apos;s pretty much infeasible to answer them all in one message.
and even if you were able to do, you have to go question by question
which makes the email to have a very unnatural format.
</p>

<p>
then there&apos;s the quality problem:
some of the questions are very hard, that nobody knows the answers to.
the recipient has no idea how to respond to such a question,
and as such is very discouraged from responding at all.
</p>

<p>
i think i found a way to address both issues.
rather than asking all the questions in one letter,
i focus on asking a single question.
if i have more, then i might start tracking the list of questions or topics
that needs further discourse, and simply ask them at a later point.
furthermore i also provide 2-4 answer options
and the recipient simply must choose the least wrong answer
with an optional comment.
</p>

<p>
i don&apos;t think one needs to obsess about the answer quality
since the expectation is to choose the least wrong.
i&apos;d expect people would mention a better answer if they have in the comment.
the point is to force people to make a decision.
in case of indecisive people
you can pretty much bypass the indecisive consciousness
and get an answer right from the subconsciousness.
for your email partner the whole thing is like
one of those conversational adventure games
(e.g. telltale&apos;s walking dead or life is strange).
</p>

<p>
however the downside is that this is much harder on the questioner.
they really need to think a lot what single question would be the one
that uncovers the most information for them about the other person.
it&apos;s an intriguing puzzle game.
it&apos;s somewhat similar to debugging computer programs.
but here you are debugging humans.
you create several hypotheses
and the other person tells you which one is the closest.
or it&apos;s like that &quot;hot and cold&quot; kid game
where you hide something and the kid has to find it
with you converting their distance from it into temperatures.
</p>

<p>
it&apos;s a very slow process but if one has patience
and the other person is willing to play this honestly,
i think one can learn quite a lot of deep things about the other person.
and it&apos;s very easy to start.
you start with something like &quot;how are you these days? good/bad&quot;
and then the rest of the questions sort of give themselves.
e.g. &quot;what makes you happy/sad these days?&quot; for the above one.
if unsure, you can always ask &quot;why?&quot; for the previous answer
and try to come up with some realistic answers to it.
</p>

<p>
i think it&apos;s important to play this over email rather than instant messaging.
sometimes it might take several days
to come up with the next question or even with the answer.
email supports labeling and threading by default
so it&apos;s the best medium for slow communication.
</p>

<p>
such a discussion could be almost neverending.
however given the fact that one could go at it very slowly,
that shouldn&apos;t be too much of a problem.
one could easily talk like that with multiple people
if they are not bothered by the fact that the delays
between the individual messages with a given person will keep growing.
also, if you have lots of questions that you want to ask a single person,
you could have multiple parallel threads with them.
though i&apos;d expect that could be quite strenuous to maintain.
</p>

<p>
to give a small taste how this could look like,
i&apos;ve created a demo about this at /dialog.
it&apos;s just a dumb dialog tree.
it obviously cannot have as much nuance as a realtime conversation could have.
but it was quite fun to write it.
it was quite tiring to write it though so therefore it&apos;s very short.
</p>

<p>
btw, this idea could take many other forms.
for instance that dialog tree from above could be a collaboratively edited one,
implemented similarly like a wiki.
that way it could grow quite big.
with some sort of trusted peer review it could stay quite sensible.
or one could make an app for it,
that keeps asking you questions very slowly.
and then its developer just keep adding new nodes based on the stats
where the most people are stuck currently.
the developer&apos;s goal would be
to split people along the nodes as much as possible.
or maybe one could make a live tv interview show
where the host comes up with good questions
that both the viewers and the interviewee tries to answer.
first the interviewee tries to guess the viewers response,
and then analyze those results.
i think this could give a good insight how someone thinks
since they have to put themselves into other people&apos;s shoes.
the socractic method has similarities to this method too.
</p>

<p>
i&apos;m not sure what the name of this technique would be.
i went with &quot;optioning&quot;
since the primary task is to give options to the other person.
</p>

<p>
anyways, at least i now have a new technique in my toolset,
maybe i&apos;ll use it one day too.
</p>

<h2 id="typer">typer</h2>
<p>
i don&apos;t usually use laptops, i much prefer the static desktop setups.
but i sometimes i do dream about what my ideal mobile computer would be.
my problem with most modern devices is
that they run these ultracomplicated operating systems
running all sorts of processes on it.
it bothers me because it uses up energy and battery unnecessarily.
</p>

<p>
i wish for a machine that could last for weeks or months on a single charge.
ideally something that doesn&apos;t consume any energy if you don&apos;t interact with it.
the kindle is almost like this however it&apos;s screen has a too slow refresh rate
so it&apos;s a bit limited for interactive use.
and besides, even kindle&apos;s system is full of unnecessary stuff.
it runs linux which pretty much means it&apos;s full of unnecessary stuff.
</p>

<p>
my primary usecase for such a device would be writing and text editing.
so i definitely need a good keyboard as well.
phones and tables are out of question.
they have their place though: i think kindle is quite good for reading.
for keyboard it would be nice if it were splittable just like ergodox.
most ideal would be if the two halves could be snapped together to
act as a single keyboard like on ordinary laptops too.
the screen should be detachable too.
it should come with a small tall stand that i could put it on
so that i don&apos;t need to unnecessary hunch to use it
if i have access to a proper desk.
by detachable i don&apos;t mean wireless:
i don&apos;t mind having the screen and the keyboard wired.
</p>

<p>
as for the screen: i don&apos;t need colors.
i think my ideal screen would be those old monochrome lcd screens
that old calculators had.
those lasted quite long on very little battery.
and those worked quite nice in sunlight without any backlight too.
and i don&apos;t need a large resolution.
something like 640x480 or in that range would be more than enough.
hopefully this wouldn&apos;t drain much energy in general.
</p>

<p>
but still, what about the operating system and the cpu?
how could i have a very dumb system
but at the same time still have access to the modern world
of compilers, internet and so on?
i think this laptop needs two chips:
</p>

<ul><li>
  a microcontroller with very low power consumption
  that could run some in memory filesystem and a text editor.
  this microcontroller would run pretty much all the time.
</li><li>
  a full blown mini-computer (e.g. raspberry pi zero w).
  this would be mostly off but i can occassionally turn it on to
  sync my edits to disk, run a compilation and get back the results,
  or simply fetch my email from internet.
  ideally it would also have a 4g modem,
  so that the device would be internet enabled.
</li>
</ul>

<p>
i think this dual system could work quite perfectly.
using a dumb microcontroller for the primary interface means
that you can really minimize all the unnecessary stuff to a bare minimum
without needing to deal with the complexities of a full modern system.
and ideally this microcontroller wouldn&apos;t consume lot of energy,
nor would the keyboard or the screen,
so i could easily use this device for a long time.
sure, the text editor would have limited features, but that&apos;s fine.
i don&apos;t even need wrapping support thanks to my semantic break style.
</p>

<p>
i could even code like this.
for instance i&apos;d implement some big function.
then i&apos;d activate the mini computer.
the linux on it would quickly boot up,
write my in-memory edits to disk,
call the compiler and write its error messages back to another in-memory file,
and then simply turn itself off.
now i can fix all those errors one by one
without running the full computer, without draining the battery.
once i&apos;m happy with the changes,
i can turn the machine back on to compile again and
maybe push the changes to, say, github.
ideally the microcontroller would be programmable,
so, for example, i should be able to run a light c syntax checker on it
to save me activating the full computer before i fixed all the dumb errors.
</p>

<p>
even email reading would be quite nice (assuming people do plaintext emails).
the process would be this: turn on the mini computer,
it fetches the emails and writes them to the in-memory fs,
and then it turns itself off.
now i can read them at my pleasure, maybe even prepare some responses.
</p>

<p>
and since the battery could last for weeks
maybe i could go into the woods for a week long fasting retreat
into a forest or onto a boat for maximum silence
and this little device (along with a kindle)
could keep me quite entertained without the need of a big energy source.
</p>

<p>
and i suspect it wouldn&apos;t be too complicated to build such a device.
maybe one could hack it together with some preexisting components.
but to be fair, i don&apos;t have any hardware experience,
and i don&apos;t really have a big need for such a device
(i&apos;m not mobile -- i spend almost all my life sitting at home)
so i&apos;m unlikely to ever attempt to create such a device.
i&apos;m just saying it would be nice if it existed.
</p>

<h2 id="briefquiz">briefquiz</h2>
<p>
in /challenge i mentioned an idea of a challenge event i could run.
but lately i was thinking a bit about it.
perhaps that idea is a bit too impersonal, too steep as a starting point.
i remembered how i all started developing my interest in computers
at about an age of 10.
there was actually a &quot;competition&quot; aimed at my level.
you could sign up for it
and then they would send you some questions via snail mail.
then you would anwser them and send the answers back via snail mail.
i started participating in this.
but when i was starting, i didn&apos;t really know the answers.
so one of the infotech teachers in the school started helping me.
we worked through the problems together.
</p>

<p>
two things happened here:
first, the letter format was a very good,
soft introduction to the world of computing.
second, i basically got a mentor out of this.
i spent a lot of time at that teacher.
i ran to him even between classes so that he explain some questions to me.
i&apos;m not sure why he invested so much time in me
(i took away his breaks after all),
but i&apos;m glad he did, since it helped me significantly.
</p>

<p>
so based on these observations and my previous ideas,
i think i&apos;d want to make a challenge site that consists of two parts.
there&apos;s a &quot;qualification&quot; phase that is done purely through email,
and there&apos;s a &quot;challenge&quot; phase
where the participants demonstrate their skills in person
a bit similarly to what i described in /challenge.
</p>

<p>
i think the qualification phase is important
to light the fire in the kids&apos; hearts.
i imagine it as follows:
a kid (or even adults if they wish) would sign up for my site.
then i&apos;d send the kid some easy questions.
e.g.:
</p>

<ul><li>
  what was the first computer?
</li><li>
  what are the computers good at doing?
</li><li>
  how do you look up information when you don&apos;t know something?
</li><li>
  how would you determine how many characters a piece of text has?
</li><li>
  you have a file with 1000 numbers.
  how would you create another file that has the each of those numbers doubled?
</li>
</ul>

<p>
then on later rounds the questions would become harder:
</p>

<ul><li>
  what is a programming language?
</li><li>
  what is a shell (e.g. bash)?
</li><li>
  what is a shell script?
</li><li>
  what is linux?
</li><li>
  where is javascript used?
</li><li>
  what is compilation?
</li><li>
  how would you compile and run some c code?
</li><li>
  write a hello world c program!
</li>
</ul>

<p>
and stuff like that.
about 2-4 questions per round.
i&apos;d expect that the kids send the answer back in an email.
and i&apos;d limit the answer to 300 characters,
they would need to fit their answers into that limit.
</p>

<p>
the kids can use any help they need, i wouldn&apos;t really care.
and then i would answer each email individually.
depending on the letter i might even get a bit philosophical:
for example if for that information lookup question i get a googling answer,
i might respond that using teachers and mentors is also a good source,
and for some type of questions and problems, it&apos;s the only source.
if they didn&apos;t answer the questions correctly,
i might ask them why did that happen,
otherwise i&apos;d just send them the next set of questions.
</p>

<p>
if they persist through the qualification rounds (about 4-5 rounds)
i&apos;d introduce them to the &quot;challenges&quot; where you have actual coding tasks.
i&apos;d create 90 static &quot;challenges&quot;.
90 because i want to refer to the challenges as a number starting from 10.
keep in mind that these would be relatively easy challenges,
wouldn&apos;t go deep into the algorithms.
the idea is to keep them relatively easily achievable,
so that the kids can get a sense of achievement,
that would lead them to start studying these things deeper,
and maybe go on to the more mature national contests
to further hone their skills.
and the challenges have to remain relatively easy,
so that their cs teacher can easily work with the kids.
</p>

<p>
i&apos;d make about 10 shell scripting tasks,
about 40 c tasks, and about 40 javascript tasks.
i think i can cover most important things with that.
the c and javascript challenges would be very similar
so basically i would be asking them to implement the same thing
in two entirely different languages.
the challenges themselves would be public,
anybody can read and prepare for them on their own.
think of relatively easy tasks,
like guess what number i&apos;m thinking or
draw a large ascii art christmas tree.
</p>

<p>
however the act of solving these tasks wouldn&apos;t be conducted online.
we&apos;d probably go to a computer lab after school together,
and i&apos;d expect that the kid demonstrates writing the solution without any help.
i wouldn&apos;t allow any references.
they shall memorize the syntax and the function parameters.
on the other hand they can try any time and retry any number of times.
so there&apos;s no pressure.
and to save time, they can do any number of challenges per session.
even one challenge per session is fine.
the challenges are intended to be easy enough
that one can solve multiple ones in a short amount of time,
depending on their experience.
if they manage to solve the problem without help,
i&apos;ll consider their challenge complete.
they can now either try another one,
or go home to prepare for the next one.
</p>

<p>
i&apos;ll keep their solution for myself,
and after the session i&apos;d send them a review of their solution
or suggest improvements.
and if somebody manages to get through all the 90 challenges,
i&apos;ll add them to a &quot;hall of fame&quot; somewhere on the internet
or maybe even to a physical poster somewhere (e.g. on a school wall).
</p>

<p>
i also figured out how could i start the whole process:
when my kid becomes school age,
i&apos;m pretty sure there will be parent-teacher meetings.
i could ask the teachers on such occassions
if they were willing to advertise my challenges to the older kids.
starting with a single school is fine,
and from that on, i&apos;m hoping to rely on a word of mouth.
if that doesn&apos;t work out then i need to improve my product until it does.
i wouldn&apos;t want more few kids signing up for this though.
more than a dozen would be too much.
if i get more, i&apos;d just increase the difficulty of the qualification questions
to cull the less motivated ones.
</p>

<p>
or maybe rather than using emails,
i should stick to letters via snail mail
to cull out the less motivated ones (the ones lazy to go to the post office).
physical letter exchange might actually give it some extra excitement,
and it also allows me respond relatively slowly,
so that i can keep the pace of the whole thing is relatively slow.
this also requires some token amount of money to participate,
so i would be somewhat protected from spam
or from a suddenly very high subscription rate.
</p>

<p>
and i could run this here in switzerland even if i don&apos;t speak german well.
i can just keep everything in english
and let the kids deal with the english rather than me with the german.
at least they would have some extra motivation to learn english.
although i&apos;d probably accept german answers too but i wouldn&apos;t document this.
</p>

<p>
and i would only accept people from my local city.
i don&apos;t really want to deal with people that i can&apos;t meet personally.
i just want to give some interactive game for the local community.
</p>

<p>
more specifically i just want to give opportunity to kids
similar to what i received as a kid and more.
i very much wanted to get my schoolmates into coding too,
but nobody was that much interested.
even if i managed to get some people into this,
the competitions were way too fierce for them to keep along for long.
my hope with such a forgiving place would be to keep them interested.
the only way to lose is to give up
rather than being plain unlucky or not smart enough.
the kids can even prepare each other for my exams.
</p>

<p>
i know that there are nice online communities like khan academy.
but i can compare my experience with online and offline competitions:
offline competitions, one where i actually am next to the others
were usually a more exhilarating experience.
a person giving you some recognition in front of others feels more exciting
than watching a robot telling you that you did good
while you sit alone in a dark room, isolated from the world.
i just want to try giving such an experience for the next generation.
i think this idea could totally give that with a really little effort.
but it still requires effort from my part, so we&apos;ll see if i ever start this.
</p>

<h2 id="goldentesting">goldentesting</h2>
<p>
ugh, i really hate unittests.
they might look nice for some trivial cases
but for modules requiring more complicated setups and dependencies
they just feel like boring busywork.
and every time i want to make a change,
i&apos;m now required to maintain the unittests too.
sure, they catch an error sometimes,
but more often i&apos;m making an intentional change,
and now i have to implement that change at multiple places.
i often dread making changes
because i don&apos;t want to deal with the weird unittests some projects have.
</p>

<p>
but here&apos;s the good news:
i firmly believe all this unittesting madness will go away
when people learn that there is a much better alternative: goldentesting.
i think that&apos;s the most common term for this.
but i heard the terms of output testing or gold master testing too.
the primary reason it&apos;s not super common yet is
that there&apos;s no good generic tooling for it.
but i&apos;m pretty sure tools will slowly get there.
though it&apos;s used in some niche areas for a very long time already.
</p>

<p>
the unittest idea is this: you write code and litter it with assertions.
the goldentest idea is this: you write code and emit text to stdout.
during code review you just need to review the diffs,
you don&apos;t need to maintain the assertions manually.
</p>

<p>
suppose you write some c++ class like this:
</p>

<pre>
  class mystring { ... };
</pre>

<p>
unittests could look like this (oversimplified):
</p>

<pre>
  int main() {
    mystring s(&quot;helló&quot;);
    assert(s.length() == 6);
    assert(s.size() == 6);
    return 0;
  }
</pre>

<p>
goldentests would look like this (oversimplified):
</p>

<pre>
  int main() {
    mystring s(&quot;helló&quot;);
    printf(&quot;%s length %d\n&quot;, s.c_str(), (int)s.length());
    printf(&quot;%s size %d\n&quot;, s.c_str(), (int)s.size());
    return 0;
  }
</pre>

<p>
running this code would then output this:
</p>

<pre>
  helló length 6
  helló size 6
</pre>

<p>
you might be inclined to commit this output next to the code,
but that&apos;s a bit spammy.
however this is where better tooling could come handy:
that tool could run the test binary at both old and new versions,
and then just present you the diff
that you and the reviewer must explicitly acknowledge before merging.
</p>

<p>
for example let&apos;s assume that some user comes along
and wants that the length() function should be unicode aware.
if you simply make the change in your code
then both unit and goldentests will fail.
the unittest will fail with an assertion
that you then have to manually fix.
the goldentest will present you this diff:
</p>

<pre>
  -helló length 6
  +helló length 5
   helló size 6
</pre>

<p>
all you and the reviewer needs to do here is to acknowledge the diff.
the commit itself doesn&apos;t need to be littered with the test changes.
after the merge, you don&apos;t need to worry about this diff anymore.
</p>

<p>
you don&apos;t even need to hardcode that &quot;helló&quot; string.
it can come from from the stdin.
then you can easily create multiple testcases
simply by running this test binary on multiple inputs.
</p>

<p>
what is the function of the tests even?
i believe there are two main functions:
increase our confidence that the code is correct,
and to catch inadvertent regressions.
i claim that goldentests can do both with greater ease.
</p>

<p>
how do you get confident that your code is correct in general?
you usually write some code and then check that it is doing the expected things.
with goldentests you pretty much stop here and commit.
with unittests you go one step further:
you add assertions for those expectations.
basically you are setting up a trap for future maintainers
to painstakingly maintain your assertions.
in the goldentests those assertions are still there
but rather in an implicit manner so it still achieves the same goals.
</p>

<p>
and they are just as effective catching regressions as the unittests.
you just need the right tooling
to enforce the same standards for goldentests as for the unittests.
</p>

<p>
goldentests scale much better for larger codebases.
suppose you are maintaining some logging library.
and now you suddenly want to change its output format.
more likely than not, in the unittest world there are assertions
that are asserting your very specific line format for some reason.
if you want to change the format,
then you have to create a massive change that changes all such tests too.
with goldentests this will just create a massive diff.
however chances are that the diff will be very redundant
and with some additional tooling (again, more tooling)
you would be able to canonicalize all the diffs
and you would end up with a small diff that you can easily inspect
and determine that your change doesn&apos;t change the logic, just the output format.
</p>

<p>
goldentests are more generic too.
for instance you could use it for compiler or linter warnings.
one of the generated output files could be warnings.
the diff tool for this would be smart enough to remove preexisting ones.
so whenever you are working on some code,
you would only see the new warnings that you introduce.
sure, sometimes a warning is wrong (otherwise it would be an error),
this method lets you acknowledge the warning and still commit,
without adding silly &quot;nolint&quot; comments to silence the warning forever.
the warning will be silenced automatically from the point of the commit.
if the reviewer thinks this is undesired,
they can ask the change&apos;s author
to add a todo to address the warning in the future.
this shines the best when you want to enable a new warning for your codebase.
you can simply enable the warning and ignore the resulting diff.
nobody will see that new warning for existing code,
so you are not adding a large burden on others suddenly.
the warning will only appear for new code which people will then address.
and the warning is still in the full generated file,
so if you want to clean up the whole codebase yourself,
you can just simply fix the existing instances one by one
and see that the number of warnings go down over time.
</p>

<p>
there&apos;s a practice of writing tests first, code later.
goldentests work for this too!
you can simply write down the expected output of your test app,
and then keep hacking at your library and test app code
until you get diff neutral.
you can write down the expected output without code or compiler.
maybe your teammate can just write down sample inputs and sample outputs,
and you can readily use that, no need to put it into assertions.
</p>

<p>
if you ever done coding competitions (acm icpc, topcoder, hackerrank)
then that environment is totally like this
and it&apos;s quite a satisfying environment,
especially when you see that your code passes all the tests.
when you solve a problem there,
you are usually pretty confident about your code.
and all they needed is some sample input and output text files.
furthermore all such testing is independent of the programming language.
with diff testing you can decide that you&apos;ll rewrite your slow python script
into some fast c++ code.
with unittesting it can be quite hard to see
that your rewrite had no effect on the logic.
with goldentesting all you need to verify is that the diffs are neutral
and then you&apos;ll be pretty much confident about your change.
</p>

<p>
at work i decided to write a little script
that parses a schedule file and outputs a nice text calendar
to visualise who is on duty and when.
actually, such calendar visualizations already existed,
what i really wanted is to visualize the diffs in a calendar
whenever someone is making a change to the schedule.
such a tool didn&apos;t exist before for these schedule files.
so i wrote a tool that parses and compares the old and new schedule files,
and then displays a calendar that has the differing days highlighted.
this is an example of one writing a special diffing tool:
most of the time the total state doesn&apos;t really matter,
all we care about is the diff.
but that&apos;s not even the point:
my point is that i didn&apos;t write any ordinary unittests for this tool.
all i did is that i hand written some sample inputs (old+new file pairs),
and just committed the generated output from them
since there&apos;s no good standard tooling for tracking golden diffs so far.
i kept adding inputs until i reached very high code coverage
as reported by the coverage tools.
when i reached that, i was confident that i handled most edge cases.
and obviously i carefully verified
that the output is what it should be for all the sample inputs.
</p>

<p>
it worked out great.
the most utility came after i released the tool
and people pointed out that i made a few wrong assumptions about the schedules.
so whenever i fixed a bug,
all i needed to do is to add a new sample input to cover that case.
or if it altered the output of an existing case,
it was very easy and obvious to see what was the effect of my change.
i usually don&apos;t get such a visceral feedback from unittests.
or sometimes people wanted me to alter the output a bit.
that was easy to do: i made formatting change
and in the diffs it was easy to see how the new format looked like.
i still love maintaining this piece of code
because it&apos;s super easy to make changes in it
while maintaining a high confidence in its correctness.
</p>

<p>
another example of such an environment would be
the deployment configs of the service of the team i worked at once.
this was a huge service consisting of many binaries running in many locations.
the system that runs these binaries needs the configuration
in a very denormalized manner.
so in the end we need to have (binaries * locations) number of configs.
to generate those configs we obviously use lot of templating.
there is a shared template
but then each binary must customize that (e.g. different cmdline arguments)
and then each location can have further customizations
(e.g. to test something in a single location only).
how would you test something like that?
you can&apos;t really.
e.g. you set a new cmdline flag for a binary to launch your new feature.
do you add a test into the configs that a flag is set?
that would be pretty dumb busywork.
what we had instead is that
for each change we just generated all the old+new denormalized configs
and then you inspected the diffs and verified
that your change does what you expected.
however on its own there would be massive diffs
just because of the sheer amount of binaries and locations we had.
e.g. you make a cmdline flag change in a binary&apos;s config
and now you have 30 files with a diff because we have 30 locations.
so we had a tool that canonicalized the diffs
(replaced references to location names with a placeholder string),
then it hashed the diffs
(just the lines with a diff, unchanged lines didn&apos;t matter),
and grouped the files into buckets based on their diff hash.
this worked out pretty nice since each cmdline flag was on their own line.
so what you actually need to do is to review a single diff
because then you can be sure that the rest of the diffs are the same.
if a single location has some special logic on the flag you are changing
and the diff looks different then that diff will go into a different bucket,
so you&apos;ll notice that too.
(sidenote: it&apos;s true that in this system weird overrides can silence some diffs,
but this could be combated with good discipline called &quot;weird needs to expire&quot;.
so all such weird overrides must have a clear deadline associated with them
at which point somebody will follow up and hopefully remove it.)
</p>

<p>
anyways, the point here is that i quite liked working in this system
simply because my confidence was very high in it.
the configuration pipeline was quite a mess
and sometimes very hard to understand,
but at least silly unittests weren&apos;t getting into my way
whenever i wanted to make a change.
i just made a change, looked at the diff,
and if if looked right, i knew it was fine.
this also made the review much much easier.
sometimes people implement quite complicated logic to achieve a thing.
but i don&apos;t really need to obsess too much about the logic itself.
all i need to look at the result and if it looks right,
i&apos;m not anxious about approving something that might be wrong.
even if it&apos;s wrong, i can see that it works for the cases we care about,
so it doesn&apos;t need to keep me up at night.
</p>

<p>
there&apos;s one caveat to this.
if you generate such a golden output,
make sure the output is deterministic and easy to diff.
if you want to output a list then sort it and output the entries line by line.
randomly ordered output would give you lots of spurious diffs
that no human can easily understand.
with too much info on a single line it&apos;s hard to see
where the diff begins and where it ends.
often you want to make a change but then see that the diff is hard to review.
to alleviate this issue, if it appears, you can just prepare another change
that changes the output such that
it will make your subsequent change easy to review.
e.g. you make change that sorts a previously unsorted list.
two small focused diffs are often much easier to review
than one larger one that does too many things at once.
</p>

<p>
however there is one significant area
where golden diffs are really lacking in tooling: interaction tests.
maybe you have some client server architecture
and you want to verify that the interactions between them look good.
here&apos;s how i imagine testing such a thing.
let&apos;s assume we want to test a client&apos;s interaction with a server.
ideally there would be a tool that can record the request/reply interations.
so first i&apos;d run my client against the real server
and i&apos;d record the interactions.
then there would be a tool that could act as a fake server
with some preconfigured request/reply behavior.
then during the test i&apos;d just assert
that the interactions against the fake server
are exactly the same as in the golden recorded interactions.
these interactions would be committed along the code,
since during the test it would infeasible to bring up that server.
if the test interactions don&apos;t match,
i&apos;d rerun the recording tool
to rerecord the interactions against the real server and commit that.
so in the review one could see how the interactions changed too.
this is quite similar to what happens in interaction unittests already
but there people manually copy paste the interactions into assertions.
in this solution that would be replaced by running a single tool.
</p>

<p>
now as with all things in life
the question between explicit asserts and implicit diffs as tests
is a question of tradeoffs.
assertions have their place too in some cases.
e.g. when you know there is no point in continuing some logic
then sure, assert it away.
and then maybe you can enforce that all tests must run successfully to end.
so basically you have a bit of both.
but i really hope that over time the tooling will improve just enough
that this idea will catch on and then hopefully life will be much easier
for code maintainers in general.
</p>

<p>
now it&apos;s true that acknowledging a diff is much easier
than painstakingly update an assertion or expectation.
so chances are that this might lead to more mistakes.
however i&apos;m not convinced that we should avoid mistakes at all costs.
rather, we should strive for an environment where mistakes are cheap.
changes should be rolled out progressively,
changes should be able to easily rolled back, and so on.
then we can focus more on the useful, new developments,
rather than doing busywork with maintaining silly assertions.
</p>

<p>
i&apos;ve tried talking with a few folks about this.
so far i managed to convince nobody about this idea.
fortunately this theory of mine would be quite easy to &quot;test&quot;.
i mean a sociological experiment on developers.
you devise a project that a person has to finish.
you make a version with goldentests and a version with unittests.
then you divide people into two groups:
one gets the goldentest one and is instructed to continue with that,
the other one gets the unittest one and is instructed to continue with that.
after the experiment you run a survey
on how easy it was to work with the project
and how confident are their about their code&apos;s correctness.
you also review the time it took to finish the project,
and also review their code to see how many mistakes they missed.
i predict that the goldentest group will take less time,
will be more confident about their code,
and the correctness rate will be just about equal.
the only problem is i&apos;m too lazy and inexperienced to run such an experiment.
i hope one day someone runs it and then we&apos;ll see if i was right or not.
</p>
<hr><p>the rest of the entries can be found at
<a href=https://html.notech.ie>html.notech.ie</a>.</p>
</div></body></html>
